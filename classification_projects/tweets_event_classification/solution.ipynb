{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                 id      target\ncount   7613.000000  7613.00000\nmean    5441.934848     0.42966\nstd     3137.116090     0.49506\nmin        1.000000     0.00000\n25%     2734.000000     0.00000\n50%     5408.000000     0.00000\n75%     8146.000000     1.00000\nmax    10873.000000     1.00000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7613.000000</td>\n      <td>7613.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5441.934848</td>\n      <td>0.42966</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3137.116090</td>\n      <td>0.49506</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2734.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5408.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8146.000000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>10873.000000</td>\n      <td>1.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv').fillna('')\n",
    "test_df = pd.read_csv('data/test.csv').fillna('')\n",
    "train_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df[(train_df['location'].isna()) & (train_df['target'] == 1)].count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['raw_tokens'] = train_df['text'].apply(nltk.word_tokenize)\n",
    "train_df['raw_token_count'] = train_df['raw_tokens'].apply(lambda tokens: len(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df['processed_text'] = train_df['text'].apply(lambda text: text.lower())\n",
    "train_df['processed_text'] = train_df['processed_text'].apply(lambda text: re.sub(r'http\\S+', ' ', text))\n",
    "train_df['processed_text'] = train_df['processed_text'].apply(lambda text: re.sub(r'[!@#$\\']', '', text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df['raw_tokens'] = test_df['text'].apply(nltk.word_tokenize)\n",
    "test_df['raw_token_count'] = test_df['raw_tokens'].apply(lambda tokens: len(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df['processed_text'] = test_df['text'].apply(lambda text: text.lower())\n",
    "test_df['processed_text'] = test_df['text'].apply(lambda text: re.sub(r'http\\S+', ' ', text))\n",
    "test_df['processed_text'] = test_df['text'].apply(lambda text: re.sub(r'[!@#$\\']', '', text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_text, test_text = train_df['text'], test_df['text']\n",
    "texts = pd.concat([train_text, test_text])\n",
    "# train_text_p, test_text_p = train_df['processed_text'], test_df['processed_text']\n",
    "# processed_texts = pd.concat([train_text_p, test_text_p])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 312 ms\n",
      "Wall time: 306 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_vec = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "word_vec.fit(texts)\n",
    "\n",
    "train_word_features = word_vec.transform(train_text)\n",
    "test_word_features = word_vec.transform(test_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "word_vec = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "word_vec.fit(processed_texts)\n",
    "\n",
    "train_word_features_p = word_vec.transform(train_text)\n",
    "test_word_features_p = word_vec.transform(test_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmora\\Documents\\Work\\Cloud Engineering\\MLOps_ML_Projects\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:554: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.2 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "character_vec = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(2, 8),\n",
    "    max_features=50000\n",
    ")\n",
    "\n",
    "character_vec.fit(texts)\n",
    "\n",
    "train_characters_features = character_vec.transform(train_text)\n",
    "test_characters_features = character_vec.transform(test_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "character_vec = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(2, 8),\n",
    "    max_features=50000\n",
    ")\n",
    "\n",
    "character_vec.fit(processed_texts)\n",
    "\n",
    "train_characters_features_p = character_vec.transform(train_text_p)\n",
    "test_characters_features_p = character_vec.transform(test_text_p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_features = hstack([train_characters_features, train_word_features])\n",
    "test_features = hstack([test_characters_features, test_word_features])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_features_p = hstack([train_characters_features_p, train_word_features_p])\n",
    "test_features_p = hstack([test_characters_features_p, test_word_features_p])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_word_features.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_word_features_p.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds, current:  1\n",
      "Fitting 5 folds, current:  2\n",
      "Fitting 5 folds, current:  3\n",
      "Fitting 5 folds, current:  4\n",
      "Fitting 5 folds, current:  5\n",
      "0.8720802873710753\n"
     ]
    }
   ],
   "source": [
    "train_oof = np.zeros(train_df.shape[0],)\n",
    "kf = KFold(random_state=127, shuffle=True)\n",
    "test_predictions = 0\n",
    "\n",
    "for jj, (train_index, val_index) in enumerate(kf.split(train_features)):\n",
    "    print(\"Fitting 5 folds, current: \", jj+1)\n",
    "    train_x = train_features.toarray()[train_index]\n",
    "    val_x = train_features.toarray()[val_index]\n",
    "    train_target = train_df['target'].values[train_index]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "    classifier.fit(train_x, train_target)\n",
    "    train_oof[val_index] = classifier.predict_proba(val_x)[:,1]\n",
    "    test_predictions += classifier.predict_proba(test_features)[:1] / 5\n",
    "\n",
    "print(roc_auc_score(train_df['target'], train_oof))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_oof_p = np.zeros(train_df.shape[0],)\n",
    "kf = KFold(random_state=127, shuffle=True)\n",
    "test_predictions_p = 0\n",
    "\n",
    "for jj, (train_index, val_index) in enumerate(kf.split(train_features_p)):\n",
    "    print(\"Fitting 5 folds, current: \", jj+1)\n",
    "    train_x = train_features_p.toarray()[train_index]\n",
    "    val_x = train_features_p.toarray()[val_index]\n",
    "    train_target = train_df['target'].values[train_index]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "    classifier.fit(train_x, train_target)\n",
    "    train_oof[val_index] = classifier.predict_proba(val_x)[:,1]\n",
    "    test_predictions += classifier.predict_proba(test_features)[:1] / 5\n",
    "\n",
    "print(roc_auc_score(train_df['target'], train_oof))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.33307407, 0.66692593]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}